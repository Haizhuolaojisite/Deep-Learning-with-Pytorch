Build your first convolutional neural network
=====================================================================

You're going to use the MNIST dataset as the dataset, which is made of handwritten digits from 0 to 9. 

The convolutional neural network is going to have 2 convolutional layers, each followed by a ReLU nonlinearity, and a fully connected layer. 

Remember that each pooling layer halves both the height and the width of the image, so by using 2 pooling layers,
the height and width are 1/4 of the original sizes.

For the moment, you are going to implement the __init__ method of the net.

NB: We need 2 pooling layers, but we only need to instantiate a pooling layer once, 
because each pooling layer will have the same configuration. 
==============================================================================================================================

1. torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, 
and not a single sample.

For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.

2. If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.
============================================================================================================================

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Instantiate two convolutional layers
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)
        
        # Instantiate the ReLU nonlinearity
        self.relu = nn.ReLU()
        
        # Instantiate a max pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        
        # Instantiate a fully connected layer
        self.fc = nn.Linear(7 * 7 * 10, 10)
        
        
Deduct the first size of the weights for the fully connected layers. Images start with shape (28, 28) and two pooling operators 
(each halving the size of the image) are performed. What is the size of the image fed to the input layer 
(heigh * width * number_of_channels)? 28/4 = 7

Now that you have declared all the parameters of your CNN, all you need to do is to implement the net's forward() method, 
and voila, you have your very first PyTorch CNN.

def forward(self, x):

        # Apply conv followd by relu, then in next line pool
        x = self.relu(self.conv1(x))
        x = self.pool(x)

        # Apply conv followd by relu, then in next line pool
        x = self.relu(self.conv2(x))
        x = self.pool(x)

        # Prepare the image for the fully connected layer: Transform the feature map from 4 dimensional to 2 dimensional space. 
        # The first dimension contains the batch size (-1), deduct the second dimension, 
        by multiplying the values for height, width and depth.
        
        x = x.view(-1, 7*7*10)

        # Apply the fully connected layer and return the result
        return self.fc(x)
==================================================================================================================================
  
Training Convolutional Neural Networks
==========================================================================================================================
This time however, you will train the CNN you built in the previous lesson, instead of a fully connected network. 
The packages you need have been imported for you and the network (called net) instantiated. 
The cross-entropy loss function (called criterion) and the Adam optimizer (called optimizer) are also available. 
We have subsampled the training set so that the training goes faster, and you are going to use a single epoch.


for i, data in enumerate(train_loader, 0):
    inputs, labels = data
    
    #Zero the parameter gradients
    optimizer.zero_grad()

    # Compute the forward pass
    outputs = net(inputs)
        
    # Compute the loss function
    loss = criterion(outputs,labels)
        
    # Compute the gradients
    loss.backward()
    
    # Update the weights
    optimizer.step()

Using CNNs to make predictions
======================================================================================================================

Building and training neural networks is a very exciting job (trust me, I do it every day)! 

However, the main utility of neural networks is to make predictions. 
This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions 
are extremely accurate.

we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.

Remember that torch.max() takes two arguments: -output.data - the tensor which contains the data.

Either 1 to do argmax or 0 to do max.

# Iterate over the data in the test_loader
for data in test_loader:

    # Get the image and label from data
    image, label = data

    # Make a forward pass in the net with your image
    output = net(image)

    # Argmax the results of the net
    _, predicted = torch.max(output.data, 1)
    if predicted == label:
        print("Yipes, your net made the right prediction " + str(predicted))
    else:
        print("Your net prediction was " + str(predicted) + ", but the correct label is: " + str(label))

<script.py> output:
    Yipes, your net made the right prediction tensor([1])
    Yipes, your net made the right prediction tensor([9])
    Yipes, your net made the right prediction tensor([7])
    Yipes, your net made the right prediction tensor([6])
    Yipes, your net made the right prediction tensor([0])
    Yipes, your net made the right prediction tensor([5])
    Yipes, your net made the right prediction tensor([0])
    Yipes, your net made the right prediction tensor([1])
    Yipes, your net made the right prediction tensor([7])
    Yipes, your net made the right prediction tensor([3])
