Your input will be images of size (28, 28), so images containing 784 pixels. Your network will contain an input_layer (provided for you), 
a hidden layer with 200 units, and an output layer with 10 classes. 

The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, 
getting the results from the network.

# Initialize the weights of the neural network
weight_1 = torch.rand(784, 200)
weight_2 = torch.rand(200,10)

# Multiply input_layer with weight_1
hidden_1 = torch.matmul(input_layer, weight_1)

# Multiply hidden_1 with weight_2
output_layer = torch.matmul(hidden_1,weight_2)
print(output_layer)

<script.py> output:
    tensor([19789.5352, 19573.3125, 19487.0332, 20075.2402, 19955.4141, 18698.8359,
            20420.6113, 20627.8965, 19162.8379, 19688.7656])
===========================================================================
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Instantiate all 2 linear layers  
        self.fc1 = nn.Linear(784, 200)
        self.fc2 = nn.Linear(200,10)

    def forward(self, x):
      
        # Use the instantiated layers and return x
        x = self.fc1(x)
        x = self.fc2(x)
        return x
===============================================================================

Build a class for a neural network which will be used to train on the MNIST dataset. 
The dataset contains images of shape (28, 28, 1), so you should deduct the size of the input layer. 
For hidden layer use 200 units, while for output layer use 10 units (1 for each class). 
For activation function, use relu in a functional way (nn.Functional is already imported as F).

import torch
import torch.nn
import torch.nn.functional as F

# Define the class Net which inherits from nn.Module
class Net(nn.Module):
    def __init__(self):    
    	# Define all the parameters of the net
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28 * 1, 200)
        self.fc2 = nn.Linear(200,10)

    def forward(self, x):   
    	# Do the forward pass
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
 =================================================================================       

Train a Neural Network


Given the fully connected neural network (called model), and a train loader called train_loader containing the MNIST dataset

You're to train the net in order to predict the classes of digits.

You will use the Adam optimizer to optimize the network, and considering that this is a classification problem 
you are going to use cross entropy as loss function.


# Instantiate the Adam optimizer with learning rate 3e-4, and Cross-Entropy as loss function

optimizer = optim.Adam(model.parameters(), lr=3e-4)

criterion = nn.CrossEntropyLoss()
  
for batch_idx, data_target in enumerate(train_loader):
    data = data_target[0]
    target = data_target[1]
    data = data.view(-1, 28 * 28)
    
    #zero the parameter gradients
    optimizer.zero_grad()

    # Complete a forward pass
    output = model(data)

    # Compute the loss, gradients of the weights, and change the weights using the Adam optimizer.
    loss = criterion(output,target)
    loss.backward()
    optimizer.step()
============================================================================================================

Using the network to make predictions


Now that you have trained the network, use it to make predictions for the data in the testing set.
The network is called model (same as in the previous exercise), and the loader is called test_loader. 
We have already initialized variables total and correct to 0.


correct,total = 0,0

# Set the model in eval mode
model.eval()

for i, data in enumerate(test_loader, 0):
    inputs, labels = data
    
    # Put each image into a vector,Put each image into a vector using inputs.view(-1, number_of_features) 
    # where the number of features should be deducted by multiplying spatial dimensions (shape) of the image.
    
    inputs = inputs.view(-1, 28*28)
    
    # Do the forward pass and get the predictions
    
    outputs = model(inputs)
    
    # The network give us scores for each class, and we get the class with highest score (using max function) as prediction
    # We save the predictions and compute the accuracy 
    
    _, outputs = torch.max(outputs.data,1)
    
    total += labels.size(0)
    
    correct += (outputs == labels).sum().item()
    
    print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))

<script.py> output:
    The testing set accuracy of the network is: 81 %






