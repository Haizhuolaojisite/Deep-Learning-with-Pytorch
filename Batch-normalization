Dropout is used to regularize fully-connected layers. Batch-normalization is used to make the training of convolutional neural networks 

more efficient, while at the same time having regularization effects. 

You are going to implement the __init__ method of a small convolutional neural network, with batch-normalization. 
 
The feature extraction part of the CNN will contain the following modules (in order): convolution, max-pool, activation, batch-norm, 
 
convolution, max-pool, relu, batch-norm.


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Implement the sequential module for feature extraction
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),
            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))
        
        # Implement the fully connected layer for classification
        self.fc = nn.Linear(in_features=7*7*20, out_features=10)
 
