转自一个知乎问答；https://www.zhihu.com/question/29021768

 

1.为什么引入非线性激励函数？

如果不适用激励函数，那么在这种情况下每一层的输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，
与没有隐藏层效果相当，这种情况就是最原始的感知机（perceptron）了

正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了，不再是输入的线性组合，可以逼近任意函数，
最早的想法是用sigmoid函数或者tanh函数，输出有界，很容易充当下一层的输入

 

2.为什么引入Relu?

第一，采用sigmoid等函数，算激活函数时候（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相当大，
而采用Relu激活函数，整个过程的计算量节省很多

 

第二，对于深层网络，sigmoid函数反向传播时，很容易就出现梯度消失的情况（在sigmoid函数接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），
从而无法完成深层网络的训练

 

第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

 

 

当然，现在也有一些对relu的改进，比如，prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进

多加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布

 

 

一言以蔽之，其实，relu函数的作用就是增加了神经网络各层之间的非线性关系，否则，如果没有激活函数，层与层之间是简单的线性关系，每层都相当于矩阵相乘，
这样怎么能够完成我们需要神经网络完成的复杂任务，

我们利用神经网络去解决图像分割，边界探测，超分辨等问题时候，我们的输入（假设为x），与期望的输出（假设为y）之间的关系究竟是什么？
也就是y=f(x)中，f是什么，我们也不清楚，但是我们对一点很确信，那就是f不是一个简单的线性函数，应该是一个抽象的复杂的关系，
那么利用神经网络就是去学习这个关系，存放在model中，利用得到的model去推测训练集之外的数据，得到期望的结果

 

 

导语
在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数。


ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。

相比于其它激活函数来说，ReLU有以下优势：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；

而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)，使得模型的收敛速度维持在一个稳定状态。

这里稍微描述一下什么是梯度消失问题：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，

这种现象尤为明显，将导致模型收敛停滞不前。
